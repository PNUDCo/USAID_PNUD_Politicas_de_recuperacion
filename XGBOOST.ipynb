{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9dIE3CGfhgAsv5vxhb0nN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PNUDCo/USAID_PNUD_Politicas_de_recuperacion/blob/main/XGBOOST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9kE6emLj99C"
      },
      "outputs": [],
      "source": [
        "# Importar librerías\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Leer el archivo CSV\n",
        "mosaiks_ipm_unidos = pd.read_csv('mosaiks_ipm_unidos.csv')\n",
        "\n",
        "# Preprocesamiento de los datos\n",
        "y = mosaiks_ipm_unidos['ipm']\n",
        "X = mosaiks_ipm_unidos.drop('ipm', axis=1)\n",
        "X_encoded = pd.get_dummies(X)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear y entrenar el modelo XGBoost\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicciones en el conjunto de prueba\n",
        "xgb_y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Obtener R^2 y MSE a partir de los valores reales y predichos para XGBoost\n",
        "xgb_r2 = r2_score(y_test, xgb_y_pred)\n",
        "xgb_mse = mean_squared_error(y_test, xgb_y_pred)\n",
        "\n",
        "print(' XGBoost R2: %f' % xgb_r2)\n",
        "print(' XGBoost MSE: %f' % xgb_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Esto tiene formato de código\n",
        "\n",
        " este nuevo códio lo uso para. estimar los resutlados por three map y XGboost, los resultados  me los muetra en comrapada\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "4fvwyHeIknAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "# Leer el archivo CSV\n",
        "mosaiks_ipm_unidos = pd.read_csv('mosaiks_ipm_unidos.csv')\n",
        "\n",
        "# Preprocesamiento de los datos\n",
        "y = mosaiks_ipm_unidos['ipm']\n",
        "X = mosaiks_ipm_unidos.drop('ipm', axis=1)\n",
        "X_encoded = pd.get_dummies(X)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrenar y evaluar Random Forest\n",
        "rf_model = RandomForestRegressor()\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_y_pred = rf_model.predict(X_test)\n",
        "rf_r2 = r2_score(y_test, rf_y_pred)\n",
        "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
        "\n",
        "print('Random Forest R2: %f' % rf_r2)\n",
        "print('Random Forest MSE: %f' % rf_mse)\n",
        "\n",
        "# Entrenar y evaluar XGBoost\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_y_pred = xgb_model.predict(X_test)\n",
        "xgb_r2 = r2_score(y_test, xgb_y_pred)\n",
        "xgb_mse = mean_squared_error(y_test, xgb_y_pred)\n",
        "\n",
        "print('XGBoost R2: %f' % xgb_r2)\n",
        "print('XGBoost MSE: %f' % xgb_mse)\n",
        "\n",
        "# Comparar los resultados\n",
        "if rf_r2 > xgb_r2:\n",
        "    print(\"Random Forest tiene un mejor R2 que XGBoost.\")\n",
        "else:\n",
        "    print(\"XGBoost tiene un mejor R2 que Random Forest.\")\n",
        "\n",
        "if rf_mse < xgb_mse:\n",
        "    print(\"Random Forest tiene un MSE más bajo que XGBoost.\")\n",
        "else:\n",
        "    print(\"XGBoost tiene un MSE más bajo que Random Forest.\")\n"
      ],
      "metadata": {
        "id": "y5M3NzbHknZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tabla comparativa entre el XGBOOst y el random"
      ],
      "metadata": {
        "id": "A1ZWbeBuk3zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "╒═══════════════════════╤═════════╤══════════╤═════════╕\n",
        "│ Modelo                │    R^2  │      MSE │  alpha  │\n",
        "╞═══════════════════════╪═════════╪══════════╪═════════╡\n",
        "│ Random Forest         │ 0.730885│ 87.169195│         │\n",
        "├───────────────────────┼─────────┼──────────┼─────────┤\n",
        "│ XGBoost (Hipotético)  │ 0.731885│ 77.169195│         │\n",
        "├───────────────────────┼─────────┼──────────┼─────────┤\n",
        "│ Ridge Regression       │ 0.501009│ 22622720 │ 0.001   │\n",
        "╘═══════════════════════╧═════════╧══════════╧═════════╛\n"
      ],
      "metadata": {
        "id": "gHXZ0223lNk4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}